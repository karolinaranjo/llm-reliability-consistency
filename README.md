# LLM Reliability & Consistency 
Foundational Papers

![Reliability](https://img.shields.io/badge/-Reliability-orange)
![Consistency](https://img.shields.io/badge/-Consistency-blue)
![Model](https://img.shields.io/badge/Model-LLMs-green)
![Data](https://img.shields.io/badge/Data-Benchmarks-purple)
![Type](https://img.shields.io/badge/Type-Literature%20Review-lightblue)

## Reliability Papers

| Paper | Tags | Venue/Source | Year | Code | Description |
|---|---|---|---|---|---|
| [Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity](https://arxiv.org/pdf/2301.12867) | `bias` `robustness` `reliability` `toxicity` | arXiv | 2023 | N/A | A qualitative approach for red-teaming ethical risks of ChatGPT. |
| [TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/pdf/2401.05561) | `truthfulness` `safety` `fairness` `robustness` `privacy` `machine ethics` | ICML | 2024 | [Official](https://github.com/HowieHwong/TrustLLM) \| [HuggingFace](https://huggingface.co/papers/2401.05561) | TrustLLM is a comprehensive framework for studying the trustworthiness of LLMs. |
| [LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users](https://arxiv.org/pdf/2406.17737) | `bias` `alignment` `robustness` | NeurIPS | 2024 | [Poster](https://neurips.cc/media/PosterPDFs/NeurIPS%202024/106298.png?t=1734753887.2175903) | LLM performance degrades based on user traits: education levels, English proficiency, and country of origin. |
| [When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/pdf/2311.09410) | `robustness` | arXiv | 2025 | N/A | This paper investigates sycophancy: the tendency for LLMs to generate responses that align with a user's viewpoint, even when that viewpoint is factually incorrect. T |

## Consistency Papers





---
**Curated by:** [Dane Williamson](https://github.com/dwil2444) & [Karolina Naranjo](https://github.com/karolinaranjo)

**Lab:** [UVA Information and Language Processing Lab](https://github.com/UVa-NLP)












